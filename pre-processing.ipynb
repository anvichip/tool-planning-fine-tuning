{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9220649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.conda/lib/python3.12/site-packages (4.5.0)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.12/site-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/lib/python3.12/site-packages (from datasets) (2.4.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.conda/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.conda/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.conda/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.conda/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.conda/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.conda/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.conda/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.conda/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in ./.conda/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in ./.conda/lib/python3.12/site-packages (from datasets) (1.3.2)\n",
      "Requirement already satisfied: packaging in ./.conda/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.conda/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in ./.conda/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.conda/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./.conda/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in ./.conda/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in ./.conda/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./.conda/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/lib/python3.12/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.conda/lib/python3.12/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdea7035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"Vikhrmodels/tool-plannings-v0.2\", \"full_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "074b0304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tools', 'conversation'],\n",
       "        num_rows: 9546\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'tools', 'conversation'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tools', 'conversation'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9684bbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'tools', 'conversation']\n",
      "{'id': 3719, 'tools': '[{\"type\": \"function\", \"function\": {\"name\": \"xml_escape\", \"description\": \"Replaces any \\\\\"<\\\\\", \\\\\">\\\\\", or \\\\\"&\\\\\" characters in the input string with their corresponding XML entities.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"s\": {\"type\": \"string\", \"description\": \"The input string to be XML-escaped.\"}}, \"required\": [\"s\"]}}}, {\"type\": \"function\", \"function\": {\"name\": \"url_encode_list\", \"description\": \"Takes a list of strings and returns a single URL string that concatenates the strings with a \\\\\"?\\\\\" separator and URL-encodes each string.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"string_list\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"A list of strings to be URL-encoded and concatenated.\"}}, \"required\": [\"string_list\"]}}}, {\"type\": \"function\", \"function\": {\"name\": \"multiples\", \"description\": \"Generates a list of all the multiples of a number that are less than a given limit.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"number\": {\"type\": \"integer\", \"description\": \"The number to find multiples of.\"}, \"limit\": {\"type\": \"integer\", \"description\": \"The upper limit for the multiples.\"}}, \"required\": [\"number\", \"limit\"]}}}]', 'conversation': '[{\"role\": \"user\", \"content\": \"Hey, can you give me a breakdown of how to throw an awesome themed party? Like, what themes work best, and how can I set everything up to really wow my guests? I want some ideas on decorations, food, and activities that will make the party unforgettable!\"}, {\"role\": \"assistant\", \"content\": \"You’ve got a cool idea for an awesome themed party, but I just don’t have the right tools to help with that detailed planning. What I can do is assist with some technical tasks like transforming strings into XML format or encoding URLs if you\\'re working on any party invites or planning materials. I can also help generate lists of multiples for any games you might want to include. Let me know how else I can support your party prep!\"}]'}\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"train\"].column_names)\n",
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4025bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9546/9546 [00:00<00:00, 36334.98 examples/s]\n",
      "Filter: 100%|██████████| 9546/9546 [00:00<00:00, 435626.05 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 20884.85 examples/s]\n",
      "Filter: 100%|██████████| 100/100 [00:00<00:00, 69350.26 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 20838.16 examples/s]\n",
      "Filter: 100%|██████████| 100/100 [00:00<00:00, 61347.14 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtering complete. Only structured plan/explain rows kept.\n",
      "Train size: 1073\n",
      "[{\"role\": \"user\", \"content\": \"Hey, can you pick a random selection of numbers for me? Let's say I want to pick 5 numbers from the range of -5 to 5.\"}, {\"role\": \"assistant\", \"content\": \"<plan>\\n1. Prepare the tool call for `random_selection` with \\\\( k = 5 \\\\).\\n2. Execute the tool call to get the random selection of numbers.\\n</plan>\\n<explain>\\nTo fulfill the user's request for a random selection of numbers, I will use the `random_selection` tool. The user specified that they want to pick 5 numbers from the range of -5 to 5. This means I will set the parameter \\\\( k \\\\) to 5, which indicates both the range and the number of selections.\\n\\nHere are the steps I will take:\\n</explain>\"}, {\"role\": \"tool\", \"tool_call_id\": 0, \"content\": {\"data\": [5, 2, -1, 4, -4]}}, {\"role\": \"assistant\", \"content\": \"The random selection of numbers from the range of -5 to 5 is: [5, 2, -1, 4, -4].\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "STEP_RE = re.compile(\n",
    "    r'^\\s*(?:[-*]\\s+|(?:(\\d+)[\\.\\)]\\s+))(.+?)\\s*$',\n",
    "    flags=re.MULTILINE\n",
    ")\n",
    "\n",
    "def ensure_messages_list(messages: Any) -> List[Dict[str, Any]]:\n",
    "    if isinstance(messages, list) and (len(messages) == 0 or isinstance(messages[0], dict)):\n",
    "        return messages\n",
    "    if isinstance(messages, str):\n",
    "        parsed = json.loads(messages)\n",
    "        if isinstance(parsed, list):\n",
    "            return parsed\n",
    "    raise ValueError(f\"Unexpected messages format: type={type(messages)}\")\n",
    "\n",
    "def _extract_steps(text: str) -> List[str]:\n",
    "    return [step.strip() for _, step in STEP_RE.findall(text) if step.strip()]\n",
    "\n",
    "def _extract_explain_prefix(text: str) -> str:\n",
    "    lines = text.splitlines()\n",
    "    cut_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if re.match(r'^\\s*(?:[-*]\\s+|\\d+[\\.\\)])\\s+\\S+', line):\n",
    "            cut_idx = i\n",
    "            break\n",
    "\n",
    "    prefix = \"\\n\".join(lines[:cut_idx]).strip() if cut_idx is not None else text.strip()\n",
    "    prefix = re.sub(r'\\n{3,}', '\\n\\n', prefix).strip()\n",
    "\n",
    "    if len(prefix) > 500:\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', prefix)\n",
    "        prefix = \" \".join(sentences[:2]).strip()\n",
    "\n",
    "    return prefix\n",
    "\n",
    "def parse_single_plan(tool_plan: str) -> Optional[Tuple[str, str]]:\n",
    "    \"\"\"Parses a single tool_plan string into plan and explain tags.\"\"\"\n",
    "    steps = _extract_steps(tool_plan)\n",
    "    explain = _extract_explain_prefix(tool_plan)\n",
    "\n",
    "    # Fallback removed as requested. If steps or explain are missing, return None.\n",
    "    if not steps or not explain:\n",
    "        return None\n",
    "\n",
    "    plan_lines = [f\"{i+1}. {s}\" for i, s in enumerate(steps)]\n",
    "    return \"\\n\".join(plan_lines).strip(), explain.strip()\n",
    "\n",
    "def to_tagged_content(plan_text: str, explain_text: str) -> str:\n",
    "    return f\"<plan>\\n{plan_text}\\n</plan>\\n<explain>\\n{explain_text}\\n</explain>\"\n",
    "\n",
    "def convert_row(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    messages = ensure_messages_list(row[\"conversation\"])\n",
    "    row = dict(row)\n",
    "    \n",
    "    new_messages = []\n",
    "    has_valid_plan = False # Start as False to drop non-tool rows\n",
    "\n",
    "    for m in messages:\n",
    "        if isinstance(m, dict) and m.get(\"role\") == \"assistant\" and \"tool_plan\" in m:\n",
    "            res = parse_single_plan(m[\"tool_plan\"])\n",
    "            \n",
    "            if res is None:\n",
    "                # We found a tool_plan but it was malformed\n",
    "                return {\"keep_row\": False} \n",
    "            \n",
    "            plan_text, explain_text = res\n",
    "            m = dict(m)\n",
    "            m[\"content\"] = to_tagged_content(plan_text, explain_text)\n",
    "            \n",
    "            m.pop(\"tool_plan\", None)\n",
    "            m.pop(\"tool_calls\", None)\n",
    "            has_valid_plan = True # Mark that we successfully converted a plan\n",
    "            \n",
    "        new_messages.append(m)\n",
    "\n",
    "    # Only keep the row if at least one tool_plan was processed\n",
    "    row[\"conversation\"] = json.dumps(new_messages, ensure_ascii=False)\n",
    "    row[\"keep_row\"] = has_valid_plan \n",
    "    return row\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Apply safely\n",
    "# -------------------------\n",
    "\n",
    "ds2 = {}\n",
    "\n",
    "for split in ds.keys():\n",
    "    mapped = ds[split].map(convert_row)\n",
    "\n",
    "    filtered = mapped.filter(lambda x: x[\"keep_row\"])\n",
    "    filtered = filtered.remove_columns(\"keep_row\")\n",
    "\n",
    "    ds2[split] = filtered\n",
    "\n",
    "print(\"✅ Filtering complete. Only structured plan/explain rows kept.\")\n",
    "print(\"Train size:\", len(ds2[\"train\"]))\n",
    "print(ds2[\"train\"][0][\"conversation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e92988e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Invalid key: 1 is out of bounds for size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mds2\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/RP_Work/fine-tuning/.conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:2878\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2876\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33marrow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpolars\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2877\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[32m-> \u001b[39m\u001b[32m2878\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/RP_Work/fine-tuning/.conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:2859\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2857\u001b[39m format_kwargs = format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m   2858\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2859\u001b[39m pa_subtable = \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2860\u001b[39m formatted_output = format_table(\n\u001b[32m   2861\u001b[39m     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\n\u001b[32m   2862\u001b[39m )\n\u001b[32m   2863\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/RP_Work/fine-tuning/.conda/lib/python3.12/site-packages/datasets/formatting/formatting.py:612\u001b[39m, in \u001b[36mquery_table\u001b[39m\u001b[34m(table, key, indices)\u001b[39m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    611\u001b[39m     size = indices.num_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table.num_rows\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     \u001b[43m_check_valid_index_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/RP_Work/fine-tuning/.conda/lib/python3.12/site-packages/datasets/formatting/formatting.py:552\u001b[39m, in \u001b[36m_check_valid_index_key\u001b[39m\u001b[34m(key, size)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (key < \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key + size < \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (key >= size):\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is out of bounds for size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n",
      "\u001b[31mIndexError\u001b[39m: Invalid key: 1 is out of bounds for size 0"
     ]
    }
   ],
   "source": [
    "ds2[\"test\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbff42e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 68.10ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved split 'train' to trial_2/original_data_with_plan_explain/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 0ba [00:00, ?ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved split 'valid' to trial_2/original_data_with_plan_explain/valid.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 0ba [00:00, ?ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved split 'test' to trial_2/original_data_with_plan_explain/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = \"trial_2/original_data_with_plan_explain/\"\n",
    "for split_name, split_dataset in ds2.items():\n",
    "    output_path = f\"{filepath}{split_name}.jsonl\"\n",
    "    split_dataset.to_json(output_path)\n",
    "    print(f\"Saved split '{split_name}' to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55da82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def process_xlam_to_llama32(input_file, output_file):\n",
    "    with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
    "        for line in f_in:\n",
    "            row = json.loads(line)\n",
    "            \n",
    "            # 1. Setup Tools in System Message\n",
    "            tools_json = json.loads(row.get('tools', '[]'))\n",
    "            system_content = (\n",
    "                \"You are a helpful assistant with tool calling capabilities. \"\n",
    "                f\"Functions: {json.dumps(tools_json)}\"\n",
    "            )\n",
    "            \n",
    "            messages = [{\"role\": \"system\", \"content\": system_content}]\n",
    "            \n",
    "            # 2. Safely Process Conversation\n",
    "            raw_conv = json.loads(row.get('conversation', '[]'))\n",
    "            for msg in raw_conv:\n",
    "                role = msg.get(\"role\")\n",
    "                # Use .get() with an empty string default to avoid KeyError\n",
    "                content = msg.get(\"content\", \"\") \n",
    "                \n",
    "                processed_msg = {\"role\": role, \"content\": content}\n",
    "                \n",
    "                # 3. Handle Tool Calls (The Assistant's Request)\n",
    "                # Check if 'tool_calls' exists in the source message\n",
    "                if \"tool_calls\" in msg:\n",
    "                    processed_msg[\"tool_calls\"] = msg[\"tool_calls\"]\n",
    "                    # Ensure arguments are stringified (required for Llama 3.2)\n",
    "                    for tc in processed_msg[\"tool_calls\"]:\n",
    "                        if isinstance(tc[\"function\"][\"arguments\"], dict):\n",
    "                            tc[\"function\"][\"arguments\"] = json.dumps(tc[\"function\"][\"arguments\"])\n",
    "                \n",
    "                # 4. Handle Tool Outputs (The Result)\n",
    "                if role == \"tool\" or role == \"ipython\":\n",
    "                    processed_msg[\"tool_call_id\"] = msg.get(\"tool_call_id\", \"0\")\n",
    "\n",
    "                messages.append(processed_msg)\n",
    "            \n",
    "            f_out.write(json.dumps({\"messages\": messages}) + \"\\n\")\n",
    "\n",
    "save_path = f\"trial_2/llama_format\"\n",
    "filepath = \"trial_2/original_data_with_plan_explain/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "process_xlam_to_llama32(f\"{filepath}/train.jsonl\", f\"{save_path}/train.jsonl\")\n",
    "process_xlam_to_llama32(f\"{filepath}/valid.jsonl\", f\"{save_path}/validation.jsonl\")\n",
    "process_xlam_to_llama32(f\"{filepath}/test.jsonl\", f\"{save_path}/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d2a565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
